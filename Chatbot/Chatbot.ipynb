{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset :\n",
    "The data set contains question and it's corresponding answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries and files\n",
    "\n",
    "* __Pandas__ : To get dataset in the form of dataframe\n",
    "* __nltk__ : It is a NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from tfidfvectorgenerator import TfidfVectorGenerator\n",
    "from doc2vecgenerator import Doc2VecGenerator\n",
    "from sent2vecgenerator import Sent2VecGenerator\n",
    "from bertgenerator import BertGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the model\n",
    "\n",
    "* First we'll tokenzie each word from the dataset.\n",
    "* After we tokenize, we will start cleaning up the tokens by Lemmatizing, removing the stopwords and removing the punctuations. Lemmatizing is the process of converting a word into its root form. \n",
    "* For example, words, like run, ran and running all convey the same meaning and hence don’t need to be considered as different words, lemmatizing will reduce all the words to run. \n",
    "* Stopwords represent the most frequent words used in Natural Language such as ‘a’, ‘is’,’ ‘what’ etc which do not add any value to the capability of the text classifier, so we remove them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "class FaqEngine:\n",
    "    def __init__(self, faqslist,type):\n",
    "        self.faqslist = faqslist\n",
    "        self.stemmer = LancasterStemmer()\n",
    "        self.le = LE()\n",
    "        self.vectorizers = {\"tfidf\":TfidfVectorGenerator(),\n",
    "                            \"doc2vec\":Doc2VecGenerator(),\n",
    "                            \"bert\":BertGenerator(),\n",
    "                            \"sent2vec\":Sent2VecGenerator()}\n",
    "        self.build_model(type)\n",
    "        \n",
    "        \n",
    "    # This funtion will perform the cleaning and preprocessing on the data\n",
    "    def cleanup(self, sentence):\n",
    "        print('cleanup')\n",
    "        word_tok = nltk.word_tokenize(sentence)\n",
    "        stemmed_words = [self.stemmer.stem(w) for w in word_tok]\n",
    "        return ' '.join(stemmed_words)\n",
    "        \n",
    "    # This funtion will train the model\n",
    "    # Now we need to feed some information into the chatbot so that it can answer to our queries. \n",
    "    def build_model(self,type):\n",
    "        \n",
    "        print('build_model')\n",
    "        \n",
    "        self.vectorizer = self.vectorizers[type]#TfidfVectorizer(min_df=1, stop_words='english')   \n",
    "        \n",
    "        # Read the data from csv files\n",
    "        dataframeslist = [pd.read_csv(csvfile).dropna() for csvfile in self.faqslist]\n",
    "        self.data = pd.concat(dataframeslist,  ignore_index=True)\n",
    "        self.questions = self.data['Question'].values[0]\n",
    "                \n",
    "        questions_cleaned = []\n",
    "        for question in self.questions:\n",
    "            questions_cleaned.append(self.cleanup(question)) \n",
    "            \n",
    "        X = self.vectorizer.vectorize(questions_cleaned)\n",
    "                 \n",
    "        y = self.data['Class'].values.tolist()\n",
    "        y = self.le.fit_transform(y)\n",
    "         \n",
    "        # Split the dataset into train and test dataset\n",
    "        trainx, testx, trainy, testy = tts(X, y, test_size=.25, random_state=42)\n",
    "        \n",
    "        # Apply SVC algorithm on the dataset with linear kernel and fit the data\n",
    "        self.model = SVC(kernel='linear')\n",
    "        self.model.fit(trainx, trainy)\n",
    "        print(\"SVC:\", self.model.score(testx, testy))       \n",
    "        \n",
    "    \n",
    "    # this funtion will take the user queries and answer it proper output using our trained model\n",
    "    def query(self, usr):\n",
    "        #print(\"User typed : \" + usr)\n",
    "        try:\n",
    "            # Clean the query\n",
    "            cleaned_usr = self.cleanup(usr)\n",
    "            t_usr_array = self.vectorizer.query(cleaned_usr)\n",
    "            prediction = self.model.predict(t_usr_array)[0]\n",
    "            class_ = self.le.inverse_transform([prediction])[0]\n",
    "            #print(\"Class \" + class_)\n",
    "            questionset = self.data[self.data['Class']==class_]\n",
    "            \n",
    "            #threshold = 0.7\n",
    "            cos_sims = []\n",
    "            for question in questionset['Question']:\n",
    "                cleaned_question = self.cleanup(question)\n",
    "                question_arr = self.vectorizer.query(cleaned_question)\n",
    "                sims = cosine_similarity(question_arr, t_usr_array)\n",
    "                #if sims > threshold:\n",
    "                cos_sims.append(sims)\n",
    "                \n",
    "            print(\"scores \" + str(cos_sims))                \n",
    "            if len(cos_sims) > 0:\n",
    "                ind = cos_sims.index(max(cos_sims)) \n",
    "                print(ind)\n",
    "                print(questionset.index[ind])\n",
    "                return self.data['Answer'][questionset.index[ind]]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"Could not follow your question [\" + usr + \"], Try again\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    faqslist = [\"faqs/Greetings.csv\", \"faqs/GSTFAQs.csv\"]\n",
    "    print('1')\n",
    "    faqmodel = FaqEngine(faqslist,'bert')\n",
    "    print('2')\n",
    "    response = faqmodel.query(\"Hi\")\n",
    "#     print('1')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
