{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset :\n",
    "The data set contains question and it's corresponding answers.\n",
    "\n",
    "## Step 1: Import libraries and files\n",
    "\n",
    "* __Pandas__ : To get dataset in the form of dataframe\n",
    "* __nltk__ : It is a NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create objects\n",
    "stemmer = LancasterStemmer()\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does aggregate turnover include value of inwar...</td>\n",
       "      <td>Section 2(6) of CGST Act. Aggregate turnover d...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What if the dealer migrated with wrong PAN as ...</td>\n",
       "      <td>registration would be required as partnership ...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A taxable person’s business is in many states....</td>\n",
       "      <td>is liable to register if the aggregate turnove...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can we use provisional GSTIN or do we get new ...</td>\n",
       "      <td>GSTIN (PID) should be converted into final GST...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whether trader of country liquor is required t...</td>\n",
       "      <td>the person is involved in 100% supply of goods...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Does aggregate turnover include value of inwar...   \n",
       "1  What if the dealer migrated with wrong PAN as ...   \n",
       "2  A taxable person’s business is in many states....   \n",
       "3  Can we use provisional GSTIN or do we get new ...   \n",
       "4  Whether trader of country liquor is required t...   \n",
       "\n",
       "                                              Answer    Class  \n",
       "0  Section 2(6) of CGST Act. Aggregate turnover d...  general  \n",
       "1  registration would be required as partnership ...  general  \n",
       "2  is liable to register if the aggregate turnove...  general  \n",
       "3  GSTIN (PID) should be converted into final GST...  general  \n",
       "4  the person is involved in 100% supply of goods...  general  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FAQs = pd.read_csv('faqs/GSTFAQs.csv').dropna()\n",
    "greet = pd.read_csv('faqs/Greetings.csv').dropna()\n",
    "\n",
    "data = pd.concat([FAQs, greet], ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning \n",
    "\n",
    "* First we'll tokenzie each word from the dataset.\n",
    "* After we tokenize, we will start cleaning up the tokens by Lemmatizing. Lemmatizing is the process of converting a word into its root form. \n",
    "* For example, words, like run, ran and running all convey the same meaning and hence don’t need to be  considered as different words, lemmatizing will reduce all the words to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup( sentence):\n",
    "    word_tok = nltk.word_tokenize(sentence)\n",
    "    stemmed_words = [stemmer.stem(w) for w in word_tok]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pass each question to the cleaning funtion defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_cleaned = []\n",
    "questions = data['Question'].values\n",
    "for question in questions:\n",
    "    questions_cleaned.append(cleanup(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Data preprocessing\n",
    "* Apply vecorization on the cleaned questions\n",
    "* Here we have used tfid vectorizer\n",
    "* It’ll see the unique words in the complete para or content given to it and then does one hot encoding accordingly. Also it removes the stopwords and stores the important words which might be used less but gives us more better features. And stores the frequency of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for training\n",
    "def vectorize(clean_questions):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')  \n",
    "    vectorizer.fit(clean_questions)\n",
    "    transformed_X_csr = vectorizer.transform(clean_questions)\n",
    "    transformed_X = transformed_X_csr.A # csr_matrix to numpy matrix  \n",
    "    return transformed_X, vectorizer\n",
    "\n",
    "# Vectorization for input query\n",
    "def query(clean_usr_msg, vectorizer):\n",
    "    t_usr_array= None\n",
    "    try:\n",
    "        t_usr = vectorizer.transform([clean_usr_msg])\n",
    "        t_usr_array = t_usr.toarray()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Could not follow your question [\" + usr + \"], Try again\"\n",
    "\n",
    "    return t_usr_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split data into train and test datasets\n",
    "\n",
    "Here we are spliting the data so that train dataset contains 75% of the data and test dataset contains 25% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 943)\n"
     ]
    }
   ],
   "source": [
    "X, vectorizer = vectorize(questions_cleaned)\n",
    "\n",
    "y = data['Class'].values.tolist()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the dataset into train and test dataset\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=.25, random_state=42)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the model\n",
    "\n",
    "Here we've used SVC algorithm to train our model with linear kernel and fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: 0.651685393258427\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "print(\"SVC:\", model.score(X_test, y_test))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Calculate the accuracy of our model\n",
    "\n",
    "Accuracy is nothing but to calculate out of total predictions how many prediction ww made are right.\n",
    "\n",
    "Accuracy = ( right predictions / total predictions made) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  64.04494382022472 %\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for value in range(1, len(X_test)):\n",
    "    t_usr_array = X_test[value-1 : value]\n",
    "    prediction = model.predict(t_usr_array)[0]\n",
    "    if prediction == y_test[value-1]:\n",
    "        count += 1\n",
    "        \n",
    "accuracy = (count / len(y_test)) * 100\n",
    "print('Accuracy = ',accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save model to the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the model\n",
    "\n",
    "Enter your query in the and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "usr = 'Hello'\n",
    "cleaned_usr = cleanup(usr)\n",
    "t_usr_array = query(cleaned_usr, vectorizer)\n",
    "prediction = model.predict(t_usr_array)[0]\n",
    "class_ = le.inverse_transform([prediction])[0]\n",
    "questionset = data[data['Class']==class_]\n",
    "\n",
    "cos_sims = []\n",
    "for question in questionset['Question']:\n",
    "    cleaned_question = cleanup(question)\n",
    "    question_arr = query(cleaned_question, vectorizer)\n",
    "    sims = cosine_similarity(question_arr, t_usr_array)\n",
    "    cos_sims.append(sims)\n",
    "\n",
    "if len(cos_sims) > 0:\n",
    "    ind = cos_sims.index(max(cos_sims)) \n",
    "    print(data['Answer'][questionset.index[ind]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
