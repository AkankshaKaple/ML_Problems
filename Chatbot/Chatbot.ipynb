{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For freshers, projects are the best way to highlight their data science knowledge. In fact, not just freshers, up to mid-level experienced professionals can keep their resumes updated with new, interesting projects. After all, they don't come easy. It takes a lot of time to create a project which can truly showcase the depth and breadth of your knowledge.\n",
    "\n",
    "I hope this project will help you gain much needed knowledge and help your resume get shortlisted faster. This project shows all the steps (from scratch) taken to solve a Machine Learning problem. For your understanding, I've taken a simple yet challenging data set where you can engineer features at your discretion as well.\n",
    "\n",
    "This project is most suitable for people who have a basic understanding of python and Machine Learning. Even if you are absolutely new to it, give it a try. And ask questions in Comments below. R users can refer to this equivalent R script and follow the explanation given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T05:16:49.566537Z",
     "start_time": "2019-10-14T05:16:49.559648Z"
    }
   },
   "source": [
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Process of Machine Learning Predictions\n",
    "2. Chatbot Data Set\n",
    "    - Understand the problem\n",
    "    - Hypothesis Generation\n",
    "    - Get Data\n",
    "    - Data Exploration\n",
    "    - Data Pre-Processing\n",
    "    - Feature Engineering - Create new features\n",
    "    - Model Training - XGBoost, Neural Network, Lasso\n",
    "    - Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of Machine Learning Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Keep tormenting data until it starts revealing its hidden secrets.” Yes, it can be done but there's a way around it. Making predictions using Machine Learning isn't just about grabbing the data and feeding it to algorithms. The algorithm might spit out some prediction but that's not what you are aiming for. The difference between good data science professionals and naive data science aspirants is that the former set follows this process religiously. The process is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understand the problem: \n",
    "Before getting the data, we need to understand the problem we are trying to solve. If you know the domain, think of which factors could play an epic role in solving the problem. If you don't know the domain, read about it. \n",
    "\n",
    "### 2. Hypothesis Generation: \n",
    "This is quite important, yet it is often forgotten. In simple words, hypothesis generation refers to creating a set of features which could influence the target variable given a confidence interval ( taken as 95% all the time). We can do this before looking at the data to avoid biased thoughts. This step often helps in creating new features. \n",
    "\n",
    "### 3. Get Data: \n",
    "Now, we download the data and look at it. Determine which features are available and which aren't, how many features we generated in hypothesis generation hit the mark, and which ones could be created. Answering these questions will set us on the right track. \n",
    "\n",
    "### 4. Data Exploration: \n",
    "We can't determine everything by just looking at the data. We need to dig deeper. This step helps us understand the nature of variables (skewed, missing, zero variance feature) so that they can be treated properly. It involves creating charts, graphs (univariate and bivariate analysis), and cross-tables to understand the behavior of \n",
    "features.\n",
    "\n",
    "### 5. Data Preprocessing: \n",
    "Here, we impute missing values and clean string variables (remove space, irregular tabs, data time format) and anything that shouldn't be there. This step is usually followed along with the data exploration stage. \n",
    "\n",
    "### 6. Feature Engineering: \n",
    "Now, we create and add new features to the data set. Most of the ideas for these features come during the hypothesis generation stage. \n",
    "\n",
    "### 7. Model Training: \n",
    "Using a suitable algorithm, we train the model on the given data set. \n",
    "\n",
    "### 8. Model Evaluation: \n",
    "\n",
    "Once the model is trained, we evaluate the model's performance using a suitable error metric. Here, we also look for variable importance, i.e., which variables have proved to be significant in determining the target variable. And, accordingly we can shortlist the best variables and train the model again. \n",
    "\n",
    "### 9. Model Testing: \n",
    "Finally, we test the model on the unseen data (test data) set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow this process in the project to arrive at our final predictions. Let's get started.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Understand the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this project has been taken from Bridgelabz Chatbot product. As mentioned above, the data set is simple. This project aims at predicting answer based on user's input question. I believe this problem statement is quite self-explanatory and doesn't need more explanation. Hence, we move to the next step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is going to be interesting. What factors can you think of right now which can influence chatbot question & answer class ? As you read this, I want you to write down your factors as well, then we can match them with the data set. Defining a hypothesis has two parts: Null Hypothesis (Ho) and Alternate Hypothesis(Ha). They can be understood as:\n",
    "\n",
    "Ho - There exists no impact of a particular feature on the dependent variable. Ha - There exists a direct impact of a particular feature on the dependent variable.\n",
    "\n",
    "Based on a decision criterion (say, 5% significance level), we always 'reject' or 'fail to reject' the null hypothesis in statistical parlance. Practically, while model building we look for probability (p) values. If p value < 0.05, we reject the null hypothesis. If p > 0.05, we fail to reject the null hypothesis. Some factors which I can think of that directly influence house prices are the following:\n",
    "\n",
    "Intent of the user's asked question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the data and load it in your python IDE.Yes, it's going to be one heck of a data exploration ride. But, we'll learn how to deal with so many variables. The target variable is question & answer class. As you can see the data set comprises numeric, categorical, and ordinal variables. Without further ado, let's start with hands-on coding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration is the key to getting insights from data. Practitioners say a good data exploration strategy can solve even complicated problems in a few hours. A good data exploration strategy comprises the following:\n",
    "\n",
    "1. Univariate Analysis - It is used to visualize one variable in one plot. Examples: histogram, density plot, etc.\n",
    "2. Bivariate Analysis - It is used to visualize two variables (x and y axis) in one plot. Examples: bar chart, line chart, area chart, etc.\n",
    "3. Multivariate Analysis - As the name suggests, it is used to visualize more than two variables at once. Examples: stacked bar chart, dodged bar chart, etc.\n",
    "4. Cross Tables -They are used to compare the behavior of two categorical variables (used in pivot tables as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the necessary libraries and data and start coding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and files\n",
    "\n",
    "* __NLTK__ : The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language\n",
    "* __Pandas__ : In computer programming, pandas is a software library written for the Python programming language for data manipulation and analysis and storing in a proper way. In particular, it offers data structures and operations for manipulating numerical tables and time series\n",
    "* __Numpy__ : NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "* __Sklearn__ : Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. The library is built upon the SciPy (Scientific Python) that must be installed before you can use scikit-learn.\n",
    "* __Pickle__ : Python pickle module is used for serializing and de-serializing a Python object structure. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:17.612892Z",
     "start_time": "2019-10-14T12:26:17.608632Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:17.816765Z",
     "start_time": "2019-10-14T12:26:17.615887Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "try:\n",
    "    faq = pd.read_csv('data/chatbot/chatbot_faq.csv')\n",
    "    greeting = pd.read_csv('data/chatbot/Greetings.csv')\n",
    "except (FileNotFoundError, IOError):\n",
    "    print(\"Wrong file or file path\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will concat the faq and greeting in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.292880Z",
     "start_time": "2019-10-14T12:26:17.819299Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([faq, greeting], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we read the data, we can look at the data using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.411104Z",
     "start_time": "2019-10-14T12:26:19.303185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>DomainIntent</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How long is the fellowship program?</td>\n",
       "      <td>fellowship program</td>\n",
       "      <td>duration</td>\n",
       "      <td>The program is 4 months on a full-time basis.</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much does fellowship program cost?</td>\n",
       "      <td>fellowship program</td>\n",
       "      <td>admission_fees</td>\n",
       "      <td>The program is free to the fellows. You do no...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the fellowship program?</td>\n",
       "      <td>fellowship program</td>\n",
       "      <td>fellowship program</td>\n",
       "      <td>Coding jobs with emerging tech product compani...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can the fellowship program be done remotely?</td>\n",
       "      <td>fellowship program</td>\n",
       "      <td>remote work</td>\n",
       "      <td>No! We believe that interaction with the ment...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I get in?</td>\n",
       "      <td>fellowship program</td>\n",
       "      <td>enter</td>\n",
       "      <td>You will require to register for one of our r...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Question        DomainIntent  \\\n",
       "0           How long is the fellowship program?  fellowship program   \n",
       "1        How much does fellowship program cost?  fellowship program   \n",
       "2               What is the fellowship program?  fellowship program   \n",
       "3  Can the fellowship program be done remotely?  fellowship program   \n",
       "4                              How do I get in?  fellowship program   \n",
       "\n",
       "               Intent                                             Answer  \\\n",
       "0            duration      The program is 4 months on a full-time basis.   \n",
       "1      admission_fees   The program is free to the fellows. You do no...   \n",
       "2  fellowship program  Coding jobs with emerging tech product compani...   \n",
       "3         remote work   No! We believe that interaction with the ment...   \n",
       "4               enter   You will require to register for one of our r...   \n",
       "\n",
       "     Class  \n",
       "0  general  \n",
       "1  general  \n",
       "2  general  \n",
       "3  general  \n",
       "4  general  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.477792Z",
     "start_time": "2019-10-14T12:26:19.414884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 51 rows and 5 columns\n"
     ]
    }
   ],
   "source": [
    "print ('The dataset has {0} rows and {1} columns'.format(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also check the data set information using the info() command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.560167Z",
     "start_time": "2019-10-14T12:26:19.481041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 5 columns):\n",
      "Question        51 non-null object\n",
      "DomainIntent    51 non-null object\n",
      "Intent          51 non-null object\n",
      "Answer          51 non-null object\n",
      "Class           51 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the data set has any missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.626360Z",
     "start_time": "2019-10-14T12:26:19.571827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check missing values\n",
    "data.columns[data.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.730583Z",
     "start_time": "2019-10-14T12:26:19.629135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing value counts in each of these columns\n",
    "miss = data.isnull().sum()/len(data)\n",
    "miss = miss[miss > 0]\n",
    "miss.sort_values(inplace=True)\n",
    "miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing data. In some case if there will be some missing data then we have to check the percentage of missing values in there columns \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we'll deal with outlier values, encode variables, impute missing values, and take every possible initiative which can remove inconsistencies from the data set. If you remember, we discovered that the variable GrLivArea has outlier values. Precisely, one point crossed the 4000 mark. Let's remove that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we'll tokenzie each word from the dataset.\n",
    "* After we tokenize, we will start cleaning up the tokens by Lemmatizing. Lemmatizing is the process of converting a word into its root form. \n",
    "- __Tokenizing__ : This breaks up the strings into a list of words or pieces based on a specified pattern using Regular Expressions aka RegEx. \n",
    "- eg : white brown fox = ‘white’, ‘brown’,’fox’\n",
    "- __Lemmatizing__ : Lemmatizing is the process of converting a word into its root form.\n",
    "- e.g., \"Playing\", \"Played\" = \"play\".\n",
    "\n",
    "In cleanup() function, we are first tokenizing the sentence (seperating each word in sentence) and then steeming (converting a word into its root form) and at the end combine all the words to form a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.801274Z",
     "start_time": "2019-10-14T12:26:19.734579Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup( sentence):\n",
    "    word_tok = nltk.word_tokenize(sentence)\n",
    "    stemmer = LancasterStemmer()\n",
    "    stemmed_words = [stemmer.stem(w) for w in word_tok]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pass each question to the cleaning funtion defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:19.948033Z",
     "start_time": "2019-10-14T12:26:19.803079Z"
    }
   },
   "outputs": [],
   "source": [
    "questions_cleaned = []\n",
    "questions = data['Question'].values\n",
    "for question in questions:\n",
    "    questions_cleaned.append(cleanup(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence __\"How long is the fellowship program?\"__ converted to __\"how long is the fellow program ?\"__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:26:20.008988Z",
     "start_time": "2019-10-14T12:26:19.952068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorization for training\n",
    "def vectorize(clean_questions):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')  \n",
    "    vectorizer.fit(clean_questions)\n",
    "    transformed_X_csr = vectorizer.transform(clean_questions)\n",
    "    transformed_X = transformed_X_csr.A # csr_matrix to numpy matrix  \n",
    "    return transformed_X, vectorizer\n",
    "\n",
    "# Vectorization for input query\n",
    "def query(clean_usr_msg, vectorizer):\n",
    "    t_usr_array= None\n",
    "    try:\n",
    "        t_usr = vectorizer.transform([clean_usr_msg])\n",
    "        t_usr_array = t_usr.toarray()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Could not follow your question [\" + usr + \"], Try again\"\n",
    "\n",
    "    return t_usr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:42:22.930873Z",
     "start_time": "2019-10-14T12:42:22.910118Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "X, vectorizer = vectorize(questions_cleaned)\n",
    "y = data['Class'].values.tolist()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no libraries or sets of functions you can use to engineer features. Well, there are some but not as effective. It's majorly a manual task but believe me, it's fun. Feature engineering requires domain knowledge and lots of creative ideas. The ideas for new features usually develop during the data exploration and hypothesis generation stages. The motive of feature engineering is to create new features which can help make predictions better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Data Into Training and Test Subsets\n",
    "In this step we will split our dataset into training and testing subsets (in proportion 80/20%).\n",
    "\n",
    "Training data set will be used for training of our linear model. Testing dataset will be used for validating of the model. All data from testing dataset will be new to model and we may check how accurate are model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:42:26.130893Z",
     "start_time": "2019-10-14T12:42:26.118880Z"
    }
   },
   "outputs": [],
   "source": [
    "#split the dataset into x and y\n",
    "x_data_train, x_data_test, y_data_train, y_data_test = train_test_split(\n",
    "        X, y, test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:42:27.986007Z",
     "start_time": "2019-10-14T12:42:27.928698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------------+--------------------------------+\n",
      "|        Algorithm         |      R2Score (Train/Test)      |     Accuracy (Train/Test)      |\n",
      "+--------------------------+--------------------------------+--------------------------------+\n",
      "|     LinearRegression     | 92/-68916328269667021487079424 | 92/-68916328269667021487079424 |\n",
      "|  Lasso LinearRegression  |             0/-16              |             0/-16              |\n",
      "| LassoCV LinearRegression |             85/41              |             85/41              |\n",
      "|       DecisionTree       |             92/74              |             92/74              |\n",
      "|           SVR            |             -3/-57             |             -3/-57             |\n",
      "+--------------------------+--------------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Using sklearn linear regression model\n",
    "\n",
    "algorithms = {'LinearRegression()':'LinearRegression', 'linear_model.Lasso(alpha=0.1)' : 'Lasso LinearRegression', \n",
    "              'LassoCV()' : 'LassoCV LinearRegression', 'DecisionTreeRegressor()': 'DecisionTree', 'SVR()':'SVR'}\n",
    "acc_train_test = []\n",
    "r2_train_test = []\n",
    "results =  {}\n",
    "for key, value in algorithms.items():\n",
    "        model = eval(key)\n",
    "        reg = model.fit(x_data_train,y_data_train) # training the dataset\n",
    "        y_pred_train =  reg.predict(x_data_train)  # predicting the results\n",
    "        y_pred_test =  reg.predict(x_data_test)\n",
    "        \n",
    "        r2_train_test.append([value, (r2_score(y_data_train,y_pred_train),r2_score(y_data_test,y_pred_test)), \n",
    "                              (reg.score(x_data_train,y_data_train), reg.score(x_data_test,y_data_test))])\n",
    "results.update({\"data\": r2_train_test})        \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Algorithm\", \"R2Score (Train/Test)\", \"Accuracy (Train/Test)\"]\n",
    "r2score = results['data']\n",
    "for val in range(0 , len(r2score)):\n",
    "    r2_value = str(math.floor(r2score[val][2][0]*100)) + '/' + str(math.floor(r2score[val][2][1]*100))\n",
    "    acc_value = str(math.floor(r2score[val][1][0]*100)) + '/' + str(math.floor(r2score[val][1][1]*100))\n",
    "    x.add_row([r2score[val][0], r2_value, acc_value])\n",
    "print(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save model to the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:42:29.160160Z",
     "start_time": "2019-10-14T12:42:29.149908Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('chatbot.pkl','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the model\n",
    "\n",
    "Enter your query in the and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:42:30.383366Z",
     "start_time": "2019-10-14T12:42:30.114084Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains new labels: [1.10286195]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-a628092f036f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt_usr_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_usr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_usr_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mquestionset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y contains new labels: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains new labels: [1.10286195]"
     ]
    }
   ],
   "source": [
    "usr = 'What is fellowship'\n",
    "cleaned_usr = cleanup(usr)\n",
    "t_usr_array = query(cleaned_usr, vectorizer)\n",
    "prediction = model.predict(t_usr_array)[0]\n",
    "class_ = le.inverse_transform([prediction])[0]\n",
    "questionset = data[data['Class']==class_]\n",
    "\n",
    "cos_sims = []\n",
    "for question in questionset['Question']:\n",
    "    cleaned_question = cleanup(question)\n",
    "    question_arr = query(cleaned_question, vectorizer)\n",
    "    sims = cosine_similarity(question_arr, t_usr_array)\n",
    "    cos_sims.append(sims)\n",
    "\n",
    "if len(cos_sims) > 0:\n",
    "    ind = cos_sims.index(max(cos_sims)) \n",
    "    print(data['Answer'][questionset.index[ind]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project has been created to help people understand the complete process of machine learning / data science modeling. These steps ensure that you won't miss out any information in the data set and would also help another person understand your work. I would like to thank the Kaggle community for sharing info on competition forums which helped me a lot in creating this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T12:25:28.900537Z",
     "start_time": "2019-10-14T12:25:28.886573Z"
    }
   },
   "source": [
    "<p>Hackerearth machine learning project <a href=\"https://www.hackerearth.com/practice/machine-learning/machine-learning-projects/python-project/tutorial/\">here</a></p>\n",
    "\n",
    "<p>Chatbot dataset <a href=\"http://bridgelabz.com\">here</a></p>\n",
    "\n",
    "<p>Machine learning code <a href=\"https://github.com/Deepakchawla/Machine-Learning-Problems\">here</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
