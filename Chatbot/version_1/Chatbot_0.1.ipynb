{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset :\n",
    "The data set contains question and it's corresponding answers. Here we are going to build a chatbot who will answer all the queries of the user based on the data it is trained on.\n",
    "\n",
    "## Step 1: Import libraries and files\n",
    "\n",
    "* __nltk__ : It is a NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response.\n",
    "* __Pandas__ : For analysis and manipulatin of data \n",
    "* __Numpy__ : To handel matrices and arrays of large size\n",
    "* __Sklearn__ : To easily handel machine learning operations\n",
    "* __Pickle__ : To save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn\n",
    "import nltk\n",
    "import warnings  # To ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    FAQs = pd.read_csv('chatbot_faq.csv')\n",
    "    greet = pd.read_csv('Greetings.csv')\n",
    "except (FileNotFoundError, IOError):\n",
    "    print(\"Wrong file or file path\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([FAQs, greet], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 observations from the data : \n",
      "                                       Question        DomainIntent  \\\n",
      "0           How long is the fellowship program?  fellowship program   \n",
      "1        How much does fellowship program cost?  fellowship program   \n",
      "2               What is the fellowship program?  fellowship program   \n",
      "3  Can the fellowship program be done remotely?  fellowship program   \n",
      "4                              How do I get in?  fellowship program   \n",
      "\n",
      "               Intent                                             Answer  \\\n",
      "0            duration      The program is 4 months on a full-time basis.   \n",
      "1      admission_fees   The program is free to the fellows. You do no...   \n",
      "2  fellowship program  Coding jobs with emerging tech product compani...   \n",
      "3         remote work   No! We believe that interaction with the ment...   \n",
      "4               enter   You will require to register for one of our r...   \n",
      "\n",
      "     Class  \n",
      "0  general  \n",
      "1  general  \n",
      "2  general  \n",
      "3  general  \n",
      "4  general  \n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 observations from the data : \")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning \n",
    "\n",
    "In data cleaning we remove the unwanted contents from data so that we can get better accuracy. \n",
    "Here we are converting each word in its root form. Some times we want our program to recognize that the words “register” and “registation” are just different tenses of the same verb, like for example, we have two questions \"How do I register for BridgeLabz?\" and \"What is the registration processing for BridgeLabz?\". Here we want our program to know that registration and register are same so their answers are also same, and that's the reason why we do lemmatizing.\n",
    "\n",
    "* First we'll tokenzie each word from the dataset.\n",
    "- __Tokenizing__ : This breaks up the strings into a list of words or pieces based on a specified pattern using Regular Expressions aka RegEx. \n",
    "- eg : white brown fox = ‘white’, ‘brown’,’fox’\n",
    "\n",
    "* After we tokenize, we will start cleaning up the tokens by Lemmatizing. \n",
    "- __Lemmatizing__ : Lemmatizing is the process of converting a word into its root form.\n",
    "- e.g., \"Playing\", \"Played\" = \"play\".\n",
    "\n",
    "In data_cleanup() function, we are first tokenizing the sentence (seperating each word in sentence) and then steeming (converting a word into its root form) and at the end combine all the words to form a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class Data_Cleanig:\n",
    "    def data_cleanup(self, sentence):\n",
    "        TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "        cleaned_text = re.sub(TEXT_CLEANING_RE, ' ', str(sentence).lower()).strip()\n",
    "        word_tok = nltk.word_tokenize(cleaned_text)\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(w) for w in word_tok]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "# def cleanup( sentence):\n",
    "#     word_tok = nltk.word_tokenize(sentence)\n",
    "#     stemmer = nltk.stem.lancaster.LancasterStemmer()\n",
    "#     stemmed_words = [stemmer.stem(w) for w in word_tok]\n",
    "#     return ' '.join(stemmed_words)\n",
    "cleaning = Data_Cleanig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pass each question to the cleaning funtion defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_cleaned = []\n",
    "questions = data['Question'].values\n",
    "questions\n",
    "for question in questions:\n",
    "    questions_cleaned.append(cleaning.data_cleanup(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are the questions before and after cleaning the data: \n",
      "\n",
      "How long is the fellowship program? ===> how long is the fellowship program \n",
      "\n",
      "How much does fellowship program cost? ===> how much doe fellowship program cost \n",
      "\n",
      "What is the fellowship program? ===> what is the fellowship program \n",
      "\n",
      "Can the fellowship program be done remotely? ===> can the fellowship program be done remotely \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Following are the questions before and after cleaning the data: \\n\")\n",
    "for i in range(4):\n",
    "    print(data.Question.iloc[i], \"===>\", questions_cleaned[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Data preprocessing\n",
    "- After removing unwanted data let's do some steps to make our data understandable for our program. That's why we do preprocessing.\n",
    "- Here we are dealing with text data, we can understant it but our machines can't. So we need to convert the data from text to numeric form.  \n",
    "- Vectorization :The process of converting NLP text into numbers is called vectorization in ML.\n",
    "- TF-IDF : TF-IDF stands for term frequency-inverse document frequency. It tell how important a word is in a sentence. The importance of a word depends on the number of times it occured in a sentence. To understand it, let's see each term:\n",
    "- __Term Frequency(TF)__ : How frequently a word appears in a sentence. We can measure it by an equation, \n",
    "\n",
    "- TF = __(Total number of times the word \"W\" occured in the sentence) / (Total number of words in the sentence)__\n",
    "- __Inverse Document Frequency (IDF)__ : How common is a word across all the sentences.\n",
    "- IDF = __log( (Total number of sentences) / (Number of sentences with word \"W\" in it))__\n",
    "* Apply vecorization on the cleaned questions\n",
    "* Here we have used tfidf vectorizer\n",
    "* It’ll see the unique words in the complete para or content given to it and then does one hot encoding accordingly. Also it removes the stopwords and stores the important words which might be used less but gives us more better features. And stores the frequency of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():       \n",
    "    # Vectorization for training\n",
    "    def vectorize(self, clean_questions):\n",
    "        vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=1, stop_words='english')  \n",
    "        vectorizer.fit(clean_questions)\n",
    "        transformed_X_csr = vectorizer.transform(clean_questions)\n",
    "        transformed_X = transformed_X_csr.A # csr_matrix to numpy matrix  \n",
    "        return transformed_X, vectorizer\n",
    "\n",
    "    # Vectorization for input query\n",
    "    def query(self, clean_usr_msg, vectorizer):\n",
    "        t_usr_array= None\n",
    "        try:\n",
    "            t_usr = vectorizer.transform([clean_usr_msg])\n",
    "            t_usr_array = t_usr.toarray()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return \"Could not follow your question [\" + usr + \"], Try again\"\n",
    "\n",
    "        return t_usr_array\n",
    "\n",
    "preprocessing = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_features, vectorizer = preprocessing.vectorize(questions_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    import re\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    lemma_function = nltk.stem.WordNetLemmatizer()\n",
    "    sentences = []\n",
    "    for text in data:\n",
    "        lemma = []\n",
    "        for token in text.split(' '):\n",
    "            if token not in stop_words:\n",
    "                lemma.append(lemma_function.lemmatize(token))    \n",
    "        sentences.append(\" \".join(lemma))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data :  ['How long is the fellowship program?']\n",
      "\n",
      "Cleaned data :  how long is the fellowship program\n",
      "\n",
      "After removing stopwords :  ['long fellowship program']\n",
      "\n",
      "Vectorized Data : \n",
      " [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.38688963 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.82091366 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.42002045\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Data : \", data.Question[:1].values)\n",
    "print('\\nCleaned data : ', questions_cleaned[0])\n",
    "d = remove_stopwords([questions_cleaned[0]])\n",
    "print('\\nAfter removing stopwords : ', d)\n",
    "print(\"\\nVectorized Data : \\n\", _features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split data into train and test datasets\n",
    "- Now our data is ready to feed to the program. But here we'll split the data into train and test dataset so that after training the model we can test the model on test dataset and find out how accurate are its predictions.\n",
    "* Here we are spliting the data so that train dataset contains 75% of the data and test dataset contains 25% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test dataset\n",
    "_labels = data.Class.values\n",
    "feature_train, feature_test, label_train, label_test = sklearn.model_selection.train_test_split(_features, _labels, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the model\n",
    "- Now we'll apply an machine learning algorithm to our processed data. \n",
    "- Here we've used SVC algorithm to train our model with linear kernel and fit the data. \n",
    "- SVC is a classification algorithm which will classify the category of the question.\n",
    "- Score : Returns the mean accuracy on the given test data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "model = sklearn.svm.SVC(kernel='linear')\n",
    "model.fit(feature_train, label_train)\n",
    "print(\"SVC:\", model.score(feature_test, label_test))   \n",
    "prediction = model.predict(feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save model to the pickle file\n",
    "\n",
    "- Here we are goining to save the model we trained into a pickle file, so that we can use it without training on new unseen data\n",
    "- So we just load the pickle file, extract the model and apply the data on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as f:\n",
    "    pickle.dump(cleaning, f)\n",
    "    pickle.dump(preprocessing, f)\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the model\n",
    "\n",
    "Enter your query in the and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding jobs with emerging tech product companies require confidence in everyday coding. BridgeLabz fellowship program gives engineers the chance to develop in-depth hands-on knowledge in a well-defined Tech Stack by developing real-world Apps. Also we guarantee your job with our partner product companies. \n"
     ]
    }
   ],
   "source": [
    "usr = 'What is fellowship program '\n",
    "cleaned_usr = cleaning.data_cleanup([usr])\n",
    "t_usr_array = preprocessing.query(cleaned_usr, vectorizer)\n",
    "prediction = model.predict(t_usr_array)[0]\n",
    "questionset = data[data['Class']==prediction]\n",
    "\n",
    "\n",
    "cos_sims = []\n",
    "for question in questionset['Question']:\n",
    "    cleaned_question = cleaning.data_cleanup(question)\n",
    "    question_arr = preprocessing.query(cleaned_question, vectorizer)\n",
    "    sims = sklearn.metrics.pairwise.cosine_similarity(question_arr, t_usr_array)\n",
    "    cos_sims.append(sims)\n",
    "\n",
    "if len(cos_sims) > 0:\n",
    "    ind = cos_sims.index(max(cos_sims)) \n",
    "    print(data['Answer'][questionset.index[ind]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
